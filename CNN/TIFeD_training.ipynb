{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN with Transfer Learning Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "from joblib import Parallel, delayed\n",
    "from keras.datasets import fashion_mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of worker nodes\n",
    "n_workers_list = [4, 8, 16, 32, 64, 128]\n",
    "\n",
    "# Hyperparameters H\n",
    "buffer_len = 20\n",
    "mini_batch_size = 10\n",
    "epochs = 10\n",
    "lrs_inv = [2048, 4096, 8192]\n",
    "\n",
    "# Number of images per worker\n",
    "size = 100\n",
    "\n",
    "# Number of runs\n",
    "n_runs = 100"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Min-Max quantization function\n",
    "def quantize_tensor(x, num_bits, min_val=None, max_val=None):\n",
    "    if not min_val and not max_val: \n",
    "        min_val, max_val = x.min(), x.max()\n",
    "    qmin = -2.**(num_bits-1)\n",
    "    qmax = 2.**(num_bits-1) - 1.\n",
    "    \n",
    "    x = x.astype(float)\n",
    "    x = x - min_val\n",
    "    x /= (max_val - min_val)\n",
    "    x *= (qmax - qmin)\n",
    "    x -= qmax\n",
    "    q_x = x.astype(float).round().astype(int)\n",
    "    \n",
    "    return q_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "(_, train_labels), (_, test_labels) = fashion_mnist.load_data()\n",
    "\n",
    "# Load the precomputed features on the ConvNet\n",
    "train_features = np.load(\"res/features/train_features.npy\")\n",
    "test_features = np.load(\"res/features/test_features.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Net Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-15T10:48:22.714668Z",
     "iopub.status.busy": "2023-01-15T10:48:22.714426Z",
     "iopub.status.idle": "2023-01-15T10:48:22.720930Z",
     "shell.execute_reply": "2023-01-15T10:48:22.719390Z"
    }
   },
   "outputs": [],
   "source": [
    "SHRT_MAX = 32767\n",
    "SHRT_MIN = (-SHRT_MAX - 1 )\n",
    "\n",
    "def isqrt(n):\n",
    "    x = n\n",
    "    y = (x + 1) // 2\n",
    "    while y < x:\n",
    "        x = y\n",
    "        y = (x + n // x) // 2\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-15T10:48:22.726927Z",
     "iopub.status.busy": "2023-01-15T10:48:22.726690Z",
     "iopub.status.idle": "2023-01-15T10:48:22.733055Z",
     "shell.execute_reply": "2023-01-15T10:48:22.731435Z"
    }
   },
   "outputs": [],
   "source": [
    "# DFA WEIGHTS\n",
    "def DFA_uniform(in_dim, out_dim):\n",
    "    range = isqrt((12 * SHRT_MAX) / (in_dim + out_dim))\n",
    "    return np.random.randint(-range, range, (in_dim, out_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLA tanh Activation function\n",
    "def PLA_tanh(act_in, in_dim, out_dim):\n",
    "    y_max, y_min = 128, -127\n",
    "    intervals = [128, 75, 32, -31, -74, -127]\n",
    "    slopes_inv = [y_max, 8, 2, 1, 2, 8, y_max]\n",
    "\n",
    "    act_out, act_grad_inv  = np.full((act_in.shape[0], out_dim), y_max), np.full((act_in.shape[0], out_dim), slopes_inv[0])\n",
    "\n",
    "    for i in range(len(act_in)):\n",
    "        for j in range(len(act_in[i].squeeze())):\n",
    "            val = act_in[i].squeeze()[j] // ((1 << 8) * in_dim)\n",
    "            if val < intervals[0]:\n",
    "                act_out[i][j] = val // 4 + 88\n",
    "                act_grad_inv[i][j] = slopes_inv[1]\n",
    "            if val < intervals[1]:\n",
    "                act_out[i][j] = val + 32\n",
    "                act_grad_inv[i][j] = slopes_inv[2]\n",
    "            if val < intervals[2]:\n",
    "                act_out[i][j] = val * 2\n",
    "                act_grad_inv[i][j] = slopes_inv[3]\n",
    "            if val < intervals[3]:\n",
    "                act_out[i][j] = val - 32\n",
    "                act_grad_inv[i][j] = slopes_inv[4]\n",
    "            if val < intervals[4]:\n",
    "                act_out[i][j] = val // 4 - 88\n",
    "                act_grad_inv[i][j] = slopes_inv[5]\n",
    "            if val < intervals[5]:\n",
    "                act_out[i][j] = y_min\n",
    "                act_grad_inv[i][j] = slopes_inv[6]\n",
    "    return act_out.astype(int), act_grad_inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-15T10:48:22.774551Z",
     "iopub.status.busy": "2023-01-15T10:48:22.774125Z",
     "iopub.status.idle": "2023-01-15T10:48:22.787663Z",
     "shell.execute_reply": "2023-01-15T10:48:22.785993Z"
    }
   },
   "outputs": [],
   "source": [
    "# L2 Loss Function Gradient\n",
    "def L2_gradient(y_true, net_out):\n",
    "    loss = np.zeros((y_true.shape[0], y_true.shape[1]))\n",
    "    for i in range(len(y_true)):\n",
    "        for j in range(len(y_true[i])):\n",
    "            loss[i][j] = net_out[i].squeeze()[j] - y_true[i][j]\n",
    "    return loss.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten Layer\n",
    "class FlattenLayer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self, image):\n",
    "        dimension = image.shape\n",
    "        try:\n",
    "            return image.reshape(dimension[0], dimension[1]*dimension[2]*dimension[3])\n",
    "        except:\n",
    "            return image.reshape(dimension[0], dimension[1]*dimension[2])\n",
    "\n",
    "    def backward(self, loss, lr_inv):\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-15T10:48:22.794879Z",
     "iopub.status.busy": "2023-01-15T10:48:22.794383Z",
     "iopub.status.idle": "2023-01-15T10:48:22.812688Z",
     "shell.execute_reply": "2023-01-15T10:48:22.810867Z"
    }
   },
   "outputs": [],
   "source": [
    "# FC Layer\n",
    "class FCLayer:\n",
    "    def __init__(self, in_dim, out_dim, desc = \"\", last_layer = False):\n",
    "        self.in_dim, self.out_dim = in_dim, out_dim\n",
    "        self.desc = desc\n",
    "        self.last_layer = last_layer\n",
    "        self.weights = np.zeros((in_dim, out_dim)).astype(int)\n",
    "        self.bias = np.zeros((1, out_dim)).astype(int)\n",
    "        self.DFA_weights = np.zeros((1, 1)).astype(int)\n",
    "    \n",
    "    def forward(self, fc_in):\n",
    "        self.input = fc_in\n",
    "        dot = (self.input @ self.weights) + self.bias\n",
    "        output, self.act_grad_inv = PLA_tanh(dot, self.in_dim, self.out_dim)\n",
    "        return output\n",
    "    \n",
    "    def backward(self, loss, lr_inv):   \n",
    "        d_DFA = self.compute_dDFA(loss, lr_inv)\n",
    "        weights_update = self.input.T @ d_DFA\n",
    "        weights_update = (weights_update // lr_inv).astype(int)\n",
    "        self.weights -= weights_update\n",
    "        ones = np.ones((len(d_DFA), 1)).astype(int)\n",
    "        bias_update = d_DFA.T @ ones\n",
    "        bias_update = (bias_update.T // lr_inv).astype(int)\n",
    "        self.bias -= bias_update\n",
    "        return loss\n",
    "    \n",
    "    def compute_dDFA(self, loss, lr_inv):\n",
    "        if self.last_layer:\n",
    "            d_DFA = np.floor_divide(loss, self.act_grad_inv)\n",
    "        else:\n",
    "            if self.DFA_weights.shape[0] != loss.shape[1] and  self.DFA_weights.shape[1] != self.weights.shape[1]: # 0 rows, 1 cols\n",
    "                print(\"DFA not initialized!\")\n",
    "            dot = loss @ self.DFA_weights\n",
    "            d_DFA = np.floor_divide(dot, self.act_grad_inv)\n",
    "        return d_DFA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network\n",
    "class Network:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "\n",
    "    # Add layer to network\n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    # Test\n",
    "    def tl_test(self, x_test, y_test):\n",
    "        corr = 0\n",
    "        for j in range(len(x_test)):\n",
    "            pred = self.tl_predict(x_test[j])\n",
    "            if pred == y_test[j]:\n",
    "                corr += 1\n",
    "        return corr / len(x_test) * 100\n",
    "\n",
    "    # Predict output\n",
    "    def tl_predict(self, input_data):\n",
    "        for layer in self.layers:\n",
    "            input_data = layer.forward(input_data)\n",
    "        return input_data.argmax()\n",
    "   \n",
    "    # Federated transfer learning training\n",
    "    def tl_federated_fit(self, data, target, lr_inv):\n",
    "        batch_target = np.eye(10, dtype='uint8')[target]*16\n",
    "\n",
    "        # Forward propagation\n",
    "        for layer in self.layers:\n",
    "            data = layer.forward(data)\n",
    "        fwd_out = data\n",
    "\n",
    "        # Loss gradient\n",
    "        loss = L2_gradient(batch_target, fwd_out)\n",
    "\n",
    "        # Backward propagation\n",
    "        for layer in reversed(self.layers):\n",
    "            layer.backward(loss, lr_inv)\n",
    "            \n",
    "        # Save weights of the layers\n",
    "        weights = []\n",
    "        for l in self.layers:\n",
    "            if hasattr(l, \"weights\"):\n",
    "                weights.append([np.copy(l.weights), np.copy(l.bias)])\n",
    "        return weights\n",
    "\n",
    "    # Federated transfer learning Single layers training\n",
    "    def tl_federated_fit_single_layers(self, data, target, train_layer, lr_inv):\n",
    "        batch_target = np.eye(10, dtype='uint8')[target]*16\n",
    "\n",
    "        # Forward propagation\n",
    "        for layer in self.layers:\n",
    "            data = layer.forward(data)\n",
    "        fwd_out = data\n",
    "\n",
    "        # Loss gradient\n",
    "        loss = L2_gradient(batch_target, fwd_out)\n",
    "\n",
    "        # Backward propagation only for one layer\n",
    "        self.layers[train_layer].backward(loss, lr_inv)\n",
    "            \n",
    "        # Save weights of the trained layer\n",
    "        weights = []\n",
    "        weights.append(np.copy(self.layers[train_layer].weights))\n",
    "        weights.append(np.copy(self.layers[train_layer].bias))\n",
    "        \n",
    "        return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload DFA weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload DFA weights if they exist\n",
    "try:\n",
    "    DFA_weights = np.load(\"res/dfa/DFA_weights.npy\")\n",
    "    print(\"DFA weights loaded!\")\n",
    "except FileNotFoundError:\n",
    "    print(\"DFA weights not found!\")\n",
    "    DFA_weights = [DFA_uniform(10, 50) for run in range(n_runs)]\n",
    "    # Save DFA weights\n",
    "    np.save(\"res/dfa/DFA_weights.npy\", DFA_weights)\n",
    "    print(\"DFA weights generated!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full-Network Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_train(run, lr_inv, features_batches, n_workers):\n",
    "    # Net structure\n",
    "    nets, n_layers = [], 0\n",
    "    for n in range(n_workers):\n",
    "        net = Network()\n",
    "        net.add(FCLayer(200, 50, desc=\"First\"))\n",
    "        net.add(FCLayer(50, 10, desc=\"Second\", last_layer=True))\n",
    "\n",
    "        net.layers[0].DFA_weights = DFA_weights[run]\n",
    "\n",
    "        nets.append(net)\n",
    "    \n",
    "    # Count number of trainable layers\n",
    "    for l in net.layers:\n",
    "        if hasattr(l, \"weights\"):\n",
    "            n_layers += 1\n",
    "\n",
    "    # Full-Network Training\n",
    "    # Loop over the buffers until all the dataset is used\n",
    "    test_accs = []\n",
    "    for buffer in range(len(features_batches[0][0])//buffer_len):\n",
    "        # Repeat for the number of epochs \n",
    "        for _ in range(epochs):\n",
    "            # Loop over the mini batches in the buffer\n",
    "            for mini_batch in range(buffer_len//mini_batch_size):\n",
    "                # Compute the starting and ending index of the mini batch\n",
    "                idx_start_batch = buffer * buffer_len + mini_batch * mini_batch_size\n",
    "                idx_end_batch = idx_start_batch + mini_batch_size\n",
    "\n",
    "                # Train the network and save the weights for each layer to be averaged\n",
    "                weights, average_weights = [[] for _ in range(n_workers)], [[] for _ in range(n_layers)]\n",
    "                for n in range(n_workers):\n",
    "                    # Extract the correct mini batch data and target from the training dataset\n",
    "                    data, target = features_batches[n][0][idx_start_batch:idx_end_batch], features_batches[n][1][idx_start_batch:idx_end_batch]\n",
    "                    train_res = nets[n].tl_federated_fit(data, target, lr_inv)\n",
    "                    weights[n] = train_res\n",
    "                    \n",
    "        # Average the weights of the layers\n",
    "        for l in range(n_layers):\n",
    "            w_mean = np.mean([weights[n][l][0] for n in range(n_workers)], axis=0).astype(int)\n",
    "            b_mean = np.mean([weights[n][l][1] for n in range(n_workers)], axis=0).astype(int)\n",
    "            average_weights[l] = [w_mean, b_mean]\n",
    "        # Set the computed average weights to the layers of the networks\n",
    "        for n in range(n_workers):\n",
    "            for l in range(n_layers):\n",
    "                nets[n].layers[l].weights = average_weights[l][0]\n",
    "                nets[n].layers[l].bias = average_weights[l][1]\n",
    "        # Compute the final test accuracy\n",
    "        test_accs.append(nets[0].tl_test(test_features, test_labels))\n",
    "    return {f\"{lr_inv}-{run}\": [test_accs, average_weights]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single-Layer Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sl_train(run, lr_inv, features_batches, n_workers):\n",
    "    # Net structure\n",
    "    nets, n_layers = [], 0\n",
    "    for n in range(n_workers):\n",
    "        net = Network()\n",
    "        net.add(FCLayer(200, 50, desc=\"First\"))\n",
    "        net.add(FCLayer(50, 10, desc=\"Second\", last_layer=True))\n",
    "\n",
    "        net.layers[0].DFA_weights = DFA_weights[run]\n",
    "\n",
    "        nets.append(net)\n",
    "    \n",
    "    # Count number of trainable layers\n",
    "    for l in net.layers:\n",
    "        if hasattr(l, \"weights\"):\n",
    "            n_layers += 1\n",
    "\n",
    "    # Single-layers Training\n",
    "    # Loop over the buffers until all the dataset is used\n",
    "    test_accs = []\n",
    "    for buffer in range(len(features_batches[0][0])//buffer_len):\n",
    "        # Repeat for the number of epochs \n",
    "        for _ in range(epochs):\n",
    "            # Loop over the mini batches in the buffer\n",
    "            for mini_batch in range(buffer_len//mini_batch_size):\n",
    "                # Compute the starting and ending index of the mini batch\n",
    "                idx_start_batch = buffer * buffer_len + mini_batch * mini_batch_size\n",
    "                idx_end_batch = idx_start_batch + mini_batch_size\n",
    "\n",
    "                # Train the network and save the weights for each layer to be averaged\n",
    "                weights, average_weights = [[] for _ in range(n_layers)], [[] for _ in range(n_layers)]\n",
    "\n",
    "                # Initialize the layer to be trained\n",
    "                train_layer = -1\n",
    "\n",
    "                for n in range(n_workers):\n",
    "                    if n%(n_workers//n_layers) == 0:\n",
    "                        train_layer += 1\n",
    "                    # Extract the correct mini batch data and target from the training dataset\n",
    "                    data, target = features_batches[n][0][idx_start_batch:idx_end_batch], features_batches[n][1][idx_start_batch:idx_end_batch]\n",
    "                    train_res = nets[n].tl_federated_fit_single_layers(data, target, train_layer, lr_inv)\n",
    "                    weights[train_layer].append(train_res)\n",
    "            \n",
    "        # Average the weights of the layers\n",
    "        for l in range(n_layers):\n",
    "            w_mean = np.mean([weights[l][n][0] for n in range(n_workers//n_layers)], axis=0).astype(int)\n",
    "            b_mean = np.mean([weights[l][n][1] for n in range(n_workers//n_layers)], axis=0).astype(int)\n",
    "            average_weights[l] = [w_mean, b_mean]\n",
    "            \n",
    "        # Set the computed average weights to the layers of the networks\n",
    "        for n in range(n_workers):\n",
    "            for l in range(n_layers):\n",
    "                nets[n].layers[l].weights = average_weights[l][0]\n",
    "                nets[n].layers[l].bias = average_weights[l][1]\n",
    "        # Compute the final test accuracy\n",
    "        test_accs.append(nets[0].tl_test(test_features, test_labels))\n",
    "    return {f\"{lr_inv}-{run}\": [test_accs, average_weights]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TIFeD Full-Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n_workers in n_workers_list:\n",
    "\n",
    "    # Split the train features into different batches\n",
    "    features_batches = [[] for _ in range(n_workers)]\n",
    "    for n in range(n_workers):\n",
    "        features_batches[n] = [train_features[n*size:(n+1)*size], train_labels[n*size:(n+1)*size]]\n",
    "\n",
    "    # Parallelization of the training procedure using joblib \n",
    "    fn_train_res = Parallel(n_jobs=-1)(delayed(fn_train)(run, lr_inv, features_batches, n_workers) for lr_inv in lrs_inv for run in range(n_runs))\n",
    "    \n",
    "    # Reorder and save the results\n",
    "    fn_train_nets = {}\n",
    "    for lr_idx in range(len(lrs_inv)):\n",
    "        accs, W = [], []\n",
    "        for run in range(n_runs):\n",
    "            # Extract the results\n",
    "            index, dict_key = n_runs*lr_idx+run, f\"{lrs_inv[lr_idx]}-{run}\"\n",
    "            accs.append(np.divide(fn_train_res[index][dict_key][0], 100))\n",
    "            W.append(fn_train_res[index][dict_key][1])\n",
    "        fn_train_nets[lrs_inv[lr_idx]] = [accs, W]\n",
    "\n",
    "    with open(f\"out/full_network/fmnist_{n_workers}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(fn_train_nets, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TIFeD Single-Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n_workers in n_workers_list:\n",
    "\n",
    "    # Split the train features into different batches\n",
    "    features_batches = [[] for _ in range(n_workers)]\n",
    "    for n in range(n_workers):\n",
    "        features_batches[n] = [train_features[n*size:(n+1)*size], train_labels[n*size:(n+1)*size]]\n",
    "\n",
    "    # Parallelization of the training procedure using joblib \n",
    "    sl_train_res = Parallel(n_jobs=-1)(delayed(sl_train)(run, lr_inv, features_batches, n_workers) for lr_inv in lrs_inv for run in range(n_runs))\n",
    "    \n",
    "    # Reorder and save the results\n",
    "    sl_train_nets = {}\n",
    "    for lr_idx in range(len(lrs_inv)):\n",
    "        accs, W = [], []\n",
    "        for run in range(n_runs):\n",
    "            # Extract the results\n",
    "            index, dict_key = n_runs*lr_idx+run, f\"{lrs_inv[lr_idx]}-{run}\"\n",
    "            accs.append(np.divide(sl_train_res[index][dict_key][0], 100))\n",
    "            W.append(sl_train_res[index][dict_key][1])\n",
    "        sl_train_nets[lrs_inv[lr_idx]] = [accs, W]\n",
    "\n",
    "    with open(f\"out/single_layers/fmnist_{n_workers}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(sl_train_nets, f)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNDVK1GUNs7ICExTzMIGeYt",
   "collapsed_sections": [],
   "name": "PyCrCNN_PocketNN_Training.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
