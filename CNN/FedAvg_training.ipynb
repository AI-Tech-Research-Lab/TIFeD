{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN with Transfer Learning Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "import copy\n",
    "import torch\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from joblib import Parallel, delayed\n",
    "from keras.datasets import fashion_mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters Configuration H\n",
    "epochs = 10\n",
    "lrs = [0.1, 0.05, 0.01]\n",
    "n_workers_list = [4, 8, 16, 32, 64, 128]\n",
    "buffer_len = 10\n",
    "mini_batch_size = 10\n",
    "\n",
    "# Number of runs\n",
    "n_runs = 100\n",
    "\n",
    "# Number of images per worker\n",
    "size = 100"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "(_, train_labels), (_, test_labels) = fashion_mnist.load_data()\n",
    "\n",
    "# Load the precomputed features on the ConvNet\n",
    "train_features = np.load(\"res/features/train_features_fp.npy\")\n",
    "test_features = np.load(\"res/features/test_features_fp.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tl_train(model, data, target, optimizer):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(data)\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    loss = loss_function(output, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Save weights of the layers\n",
    "    weights = []\n",
    "    for l in model:\n",
    "        if hasattr(l, \"weight\"):\n",
    "            weights.append(np.copy(l.weight.detach().numpy()))\n",
    "            weights.append(np.copy(l.bias.detach().numpy()))\n",
    "\n",
    "    return weights\n",
    "\n",
    "def tl_test(model, test_features, test_labels):\n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    correct = 0\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    with torch.no_grad():\n",
    "        for idx in range(len(test_labels)):\n",
    "            data = torch.tensor(np.expand_dims(test_features[idx], axis=0))\n",
    "            target = torch.tensor(np.expand_dims(test_labels[idx], axis=0))\n",
    "            output = model(data)\n",
    "            loss += loss_function(output, target).item() # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    loss /= len(test_labels)\n",
    "    test_acc = 100. * correct / len(test_labels)\n",
    "    \n",
    "    return test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallel_tl_train(run, lr, features_batches, n_workers):\n",
    "    # Net structure\n",
    "    nets, train_layers = [], []\n",
    "    net = nn.Sequential(\n",
    "            nn.Linear(200, 50),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(50, 10)\n",
    "        )\n",
    "    for n in range(n_workers):\n",
    "        nets.append(copy.deepcopy(net))\n",
    "    \n",
    "    # Trainable layers\n",
    "    for l in range(len(net)):\n",
    "        if hasattr(net[l], \"weight\"):\n",
    "            train_layers.append(l)\n",
    "    \n",
    "    # Loop over the buffers until all the dataset is used\n",
    "    test_accs = []\n",
    "    for mini_batch in range(len(features_batches[0][0])//mini_batch_size):\n",
    "        # Repeat for the number of epochs \n",
    "        for _ in range(epochs):\n",
    "            # Loop over the mini batches in the buffer\n",
    "            idx_start = mini_batch * mini_batch_size\n",
    "            idx_end = idx_start + mini_batch_size\n",
    "\n",
    "            # Train the network and save the weights for each layer to be averaged\n",
    "            weights, average_weights = [[] for _ in range(n_workers)], [[] for _ in range(len(train_layers))]\n",
    "            \n",
    "            for n in range(n_workers):\n",
    "                # Extract the correct mini batch data and target from the training dataset\n",
    "                data, target = torch.tensor(features_batches[n][0][idx_start:idx_end]), torch.tensor(features_batches[n][1][idx_start:idx_end])\n",
    "                \n",
    "                # Train params\n",
    "                optimizer = optim.SGD(nets[n].parameters(), lr=lr, momentum=0.5)\n",
    "                res = tl_train(nets[n], data, target, optimizer)\n",
    "                weights[n] = res\n",
    "\n",
    "        # Average the weights of the layers\n",
    "        for l in range(0, 2*len(train_layers)-1, 2):\n",
    "            w_mean = np.mean([weights[n][l] for n in range(n_workers)], axis=0)\n",
    "            b_mean = np.mean([weights[n][l+1] for n in range(n_workers)], axis=0)\n",
    "            average_weights[l//2] = [w_mean, b_mean]\n",
    "        \n",
    "        # Set the computed average weights to the layers of the networks\n",
    "        for n in range(n_workers):\n",
    "            for idx, l in enumerate(train_layers):\n",
    "                nets[n][l].weight = torch.nn.Parameter(torch.from_numpy(average_weights[idx][0]))\n",
    "                nets[n][l].bias = torch.nn.Parameter(torch.from_numpy(average_weights[idx][1]))\n",
    "        \n",
    "        # Compute the test accuracy\n",
    "        test_accs.append(tl_test(nets[0], test_features, test_labels))\n",
    "    return {f\"{lr}-{run}\": [test_accs, average_weights]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n_workers in n_workers_list:\n",
    "\n",
    "    # Split the train features into different batches\n",
    "    features_batches = [[] for _ in range(n_workers)]\n",
    "    for n in range(n_workers):\n",
    "        features_batches[n] = [train_features[n*size:(n+1)*size], train_labels[n*size:(n+1)*size]]\n",
    "\n",
    "    # Parallelization of the training procedure using joblib \n",
    "    fedavg_res = Parallel(n_jobs=-1)(delayed(parallel_tl_train)(run, lr, features_batches, n_workers) for lr in lrs for run in range(n_runs))\n",
    "    \n",
    "    # Reorder and save the results\n",
    "    fedavg_nets = {}\n",
    "    for lr_idx in range(len(lrs)):\n",
    "        accs, W = [], []\n",
    "        for run in range(n_runs):\n",
    "            # Extract the results\n",
    "            index, dict_key = n_runs*lr_idx+run, f\"{lrs[lr_idx]}-{run}\"\n",
    "            accs.append(np.divide(fedavg_res[index][dict_key][0], 100))\n",
    "            W.append(fedavg_res[index][dict_key][1])\n",
    "        fedavg_nets[lrs[lr_idx]] = [accs, W]\n",
    "\n",
    "    with open(f\"out/fedavg/fmnist_{n_workers}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(fedavg_nets, f)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNDVK1GUNs7ICExTzMIGeYt",
   "collapsed_sections": [],
   "name": "PyCrCNN_PocketNN_Training.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
