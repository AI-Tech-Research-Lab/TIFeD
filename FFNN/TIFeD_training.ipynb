{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FeedForward NN Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "from joblib import Parallel, delayed\n",
    "from keras.utils import to_categorical\n",
    "from keras.datasets import mnist, fashion_mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of worker nodes\n",
    "n_workers = 8\n",
    "\n",
    "# Hyperparameters H\n",
    "buffer_len = 50\n",
    "mini_batch_size = 25\n",
    "epochs = 5\n",
    "lrs_inv = [2048, 4096, 8192]\n",
    "\n",
    "# Number of runs\n",
    "n_runs = 100"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(x_train, y_train, x_test):\n",
    "    x_train, y_train = shuffle(x_train, y_train, random_state=0)\n",
    "\n",
    "    # Split the train dataset into different batches\n",
    "    images_per_batch = len(x_train) // n_workers\n",
    "    train_batches = [[0, 0] for i in range(n_workers)]\n",
    "\n",
    "    for i in range(n_workers):\n",
    "        idx = images_per_batch*i\n",
    "\n",
    "        # Scale the images to [-128, 127]\n",
    "        train_batches[i][0] = np.subtract(x_train[idx:idx+images_per_batch], 128)\n",
    "        train_batches[i][0].dtype = np.int8\n",
    "\n",
    "        # One-hot encode the labels\n",
    "        train_batches[i][1] = to_categorical(y_train[idx:idx+images_per_batch]).astype(int)*16\n",
    "\n",
    "    # Test set (scaled to [-128, 127])\n",
    "    x_test = np.subtract(x_test, 128)\n",
    "    x_test.dtype = np.int8\n",
    "\n",
    "    return train_batches, x_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Net Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-15T10:48:22.714668Z",
     "iopub.status.busy": "2023-01-15T10:48:22.714426Z",
     "iopub.status.idle": "2023-01-15T10:48:22.720930Z",
     "shell.execute_reply": "2023-01-15T10:48:22.719390Z"
    }
   },
   "outputs": [],
   "source": [
    "SHRT_MAX = 32767\n",
    "SHRT_MIN = (-SHRT_MAX - 1 )\n",
    "\n",
    "def isqrt(n):\n",
    "    x = n\n",
    "    y = (x + 1) // 2\n",
    "    while y < x:\n",
    "        x = y\n",
    "        y = (x + n // x) // 2\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-15T10:48:22.726927Z",
     "iopub.status.busy": "2023-01-15T10:48:22.726690Z",
     "iopub.status.idle": "2023-01-15T10:48:22.733055Z",
     "shell.execute_reply": "2023-01-15T10:48:22.731435Z"
    }
   },
   "outputs": [],
   "source": [
    "# DFA WEIGHTS\n",
    "def DFA_uniform(in_dim, out_dim):\n",
    "    range = isqrt((12 * SHRT_MAX) / (in_dim + out_dim))\n",
    "    return np.random.randint(-range, range, (in_dim, out_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLA tanh Activation function\n",
    "def PLA_tanh(act_in, in_dim, out_dim):\n",
    "    y_max, y_min = 128, -127\n",
    "    intervals = [128, 75, 32, -31, -74, -127]\n",
    "    slopes_inv = [y_max, 8, 2, 1, 2, 8, y_max]\n",
    "\n",
    "    act_out, act_grad_inv  = np.full((act_in.shape[0], out_dim), y_max), np.full((act_in.shape[0], out_dim), slopes_inv[0])\n",
    "\n",
    "    for i in range(len(act_in)):\n",
    "        for j in range(len(act_in[i].squeeze())):\n",
    "            val = act_in[i].squeeze()[j] // ((1 << 8) * in_dim)\n",
    "            if val < intervals[0]:\n",
    "                act_out[i][j] = val // 4 + 88\n",
    "                act_grad_inv[i][j] = slopes_inv[1]\n",
    "            if val < intervals[1]:\n",
    "                act_out[i][j] = val + 32\n",
    "                act_grad_inv[i][j] = slopes_inv[2]\n",
    "            if val < intervals[2]:\n",
    "                act_out[i][j] = val * 2\n",
    "                act_grad_inv[i][j] = slopes_inv[3]\n",
    "            if val < intervals[3]:\n",
    "                act_out[i][j] = val - 32\n",
    "                act_grad_inv[i][j] = slopes_inv[4]\n",
    "            if val < intervals[4]:\n",
    "                act_out[i][j] = val // 4 - 88\n",
    "                act_grad_inv[i][j] = slopes_inv[5]\n",
    "            if val < intervals[5]:\n",
    "                act_out[i][j] = y_min\n",
    "                act_grad_inv[i][j] = slopes_inv[6]\n",
    "    return act_out.astype(int), act_grad_inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-15T10:48:22.774551Z",
     "iopub.status.busy": "2023-01-15T10:48:22.774125Z",
     "iopub.status.idle": "2023-01-15T10:48:22.787663Z",
     "shell.execute_reply": "2023-01-15T10:48:22.785993Z"
    }
   },
   "outputs": [],
   "source": [
    "# L2 Loss Function Gradient\n",
    "def L2_gradient(y_true, net_out):\n",
    "    loss = np.zeros((y_true.shape[0], y_true.shape[1]))\n",
    "    for i in range(len(y_true)):\n",
    "        for j in range(len(y_true[i])):\n",
    "            loss[i][j] = net_out[i].squeeze()[j] - y_true[i][j]\n",
    "    return loss.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten Layer\n",
    "class FlattenLayer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self, image):\n",
    "        dimension = image.shape\n",
    "        try:\n",
    "            return image.reshape(dimension[0], dimension[1]*dimension[2]*dimension[3])\n",
    "        except:\n",
    "            return image.reshape(dimension[0], dimension[1]*dimension[2])\n",
    "\n",
    "    def backward(self, loss, lr_inv):\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-15T10:48:22.794879Z",
     "iopub.status.busy": "2023-01-15T10:48:22.794383Z",
     "iopub.status.idle": "2023-01-15T10:48:22.812688Z",
     "shell.execute_reply": "2023-01-15T10:48:22.810867Z"
    }
   },
   "outputs": [],
   "source": [
    "# FC Layer\n",
    "class FCLayer:\n",
    "    def __init__(self, in_dim, out_dim, desc = \"\", last_layer = False):\n",
    "        self.in_dim, self.out_dim = in_dim, out_dim\n",
    "        self.desc = desc\n",
    "        self.last_layer = last_layer\n",
    "        self.weights = np.zeros((in_dim, out_dim)).astype(int)\n",
    "        self.bias = np.zeros((1, out_dim)).astype(int)\n",
    "        self.DFA_weights = np.zeros((1, 1)).astype(int)\n",
    "    \n",
    "    def forward(self, fc_in):\n",
    "        self.input = fc_in\n",
    "        dot = (self.input @ self.weights) + self.bias\n",
    "        output, self.act_grad_inv = PLA_tanh(dot, self.in_dim, self.out_dim)\n",
    "        return output\n",
    "    \n",
    "    def backward(self, loss, lr_inv):   \n",
    "        d_DFA = self.compute_dDFA(loss, lr_inv)\n",
    "        weights_update = self.input.T @ d_DFA\n",
    "        weights_update = (weights_update // lr_inv).astype(int)\n",
    "        self.weights -= weights_update\n",
    "        ones = np.ones((len(d_DFA), 1)).astype(int)\n",
    "        bias_update = d_DFA.T @ ones\n",
    "        bias_update = (bias_update.T // lr_inv).astype(int)\n",
    "        self.bias -= bias_update\n",
    "        return loss\n",
    "    \n",
    "    def compute_dDFA(self, loss, lr_inv):\n",
    "        if self.last_layer:\n",
    "            d_DFA = np.floor_divide(loss, self.act_grad_inv)\n",
    "        else:\n",
    "            if self.DFA_weights.shape[0] != loss.shape[1] and  self.DFA_weights.shape[1] != self.weights.shape[1]: # 0 rows, 1 cols\n",
    "                print(\"DFA not initialized!\")\n",
    "            dot = loss @ self.DFA_weights\n",
    "            d_DFA = np.floor_divide(dot, self.act_grad_inv)\n",
    "        return d_DFA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network\n",
    "class Network:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "\n",
    "    # Add layer to network\n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    # Test\n",
    "    def test(self, x_test, y_test):\n",
    "        corr = 0\n",
    "        for j in range(len(x_test)):\n",
    "            pred = self.predict(x_test[j])\n",
    "            if pred == y_test[j]:\n",
    "                corr += 1\n",
    "        return corr / len(x_test) * 100\n",
    "\n",
    "    # Predict output\n",
    "    def predict(self, input_data):\n",
    "        output = np.expand_dims(input_data, axis=0)\n",
    "        for layer in self.layers:\n",
    "            output = layer.forward(output)\n",
    "        return output.argmax()\n",
    "   \n",
    "    # Federated training\n",
    "    def federated_fit(self, data, target, lr_inv):\n",
    "        # Forward propagation\n",
    "        for layer in self.layers:\n",
    "            data = layer.forward(data)\n",
    "        fwd_out = data\n",
    "\n",
    "        # Loss gradient\n",
    "        loss = L2_gradient(target, fwd_out)\n",
    "\n",
    "        # Backward propagation\n",
    "        for layer in reversed(self.layers):\n",
    "            layer.backward(loss, lr_inv)\n",
    "            \n",
    "        # Save weights of the layers\n",
    "        weights = []\n",
    "        for l in self.layers:\n",
    "            if hasattr(l, \"weights\"):\n",
    "                weights.append([np.copy(l.weights), np.copy(l.bias)])\n",
    "        return weights\n",
    "\n",
    "    # Federated Single layers training\n",
    "    def federated_fit_single_layers(self, data, target, train_layer, lr_inv):\n",
    "        # Forward propagation\n",
    "        for layer in self.layers:\n",
    "            data = layer.forward(data)\n",
    "        fwd_out = data\n",
    "\n",
    "        # Loss gradient\n",
    "        loss = L2_gradient(target, fwd_out)\n",
    "\n",
    "        # Backward propagation only for one layer\n",
    "        self.layers[train_layer].backward(loss, lr_inv)\n",
    "            \n",
    "        # Save weights of the trained layer\n",
    "        weights = []\n",
    "        weights.append(np.copy(self.layers[train_layer].weights))\n",
    "        weights.append(np.copy(self.layers[train_layer].bias))\n",
    "        \n",
    "        return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload DFA weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DFA weights loaded!\n"
     ]
    }
   ],
   "source": [
    "# Upload DFA weights if they exist\n",
    "try:\n",
    "    DFA_weights = np.load(\"res/dfa/DFA_weights.npy\")\n",
    "    print(\"DFA weights loaded!\")\n",
    "except FileNotFoundError:\n",
    "    print(\"DFA weights not found!\")\n",
    "    DFA_weights = [DFA_uniform(10, 200) for run in range(n_runs)]\n",
    "    # Save DFA weights\n",
    "    np.save(\"res/dfa/DFA_weights.npy\", DFA_weights)\n",
    "    print(\"DFA weights generated!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full-Network Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_train(run, lr_inv, train_batches, x_test, y_test):\n",
    "    # Net structure\n",
    "    nets, n_layers = [], 0\n",
    "    for n in range(n_workers):\n",
    "        net = Network()\n",
    "        net.add(FlattenLayer())\n",
    "        net.add(FCLayer(28*28, 200, desc=\"First\"))\n",
    "        net.add(FCLayer(200, 10, desc=\"Second\", last_layer=True))\n",
    "\n",
    "        net.layers[1].DFA_weights = DFA_weights[run]\n",
    "\n",
    "        nets.append(net)\n",
    "    \n",
    "    # Count number of trainable layers\n",
    "    for l in net.layers:\n",
    "        if hasattr(l, \"weights\"):\n",
    "            n_layers += 1\n",
    "\n",
    "    # Full-Network Training\n",
    "    # Loop over the buffers until all the dataset is used\n",
    "    for buffer in range(len(train_batches[0][0])//buffer_len):\n",
    "        # Repeat for the number of epochs \n",
    "        for _ in range(epochs):\n",
    "            # Loop over the mini batches in the buffer\n",
    "            for mini_batch in range(buffer_len//mini_batch_size):\n",
    "                # Compute the starting and ending index of the mini batch\n",
    "                idx_start_batch = buffer * buffer_len + mini_batch * mini_batch_size\n",
    "                idx_end_batch = idx_start_batch + mini_batch_size\n",
    "\n",
    "                # Train the network and save the weights for each layer to be averaged\n",
    "                weights, average_weights = [[] for _ in range(n_workers)], [[] for _ in range(n_layers)]\n",
    "\n",
    "                for n in range(n_workers):\n",
    "                    # Extract the correct mini batch data and target from the training dataset\n",
    "                    data, target = train_batches[n][0][idx_start_batch:idx_end_batch], train_batches[n][1][idx_start_batch:idx_end_batch]\n",
    "                    train_res = nets[n].federated_fit(data, target, lr_inv)\n",
    "                    weights[n] = train_res\n",
    "                    \n",
    "            # Average the weights of the layers\n",
    "            for l in range(n_layers):\n",
    "                w_mean = np.mean([weights[n][l][0] for n in range(n_workers)], axis=0).astype(int)\n",
    "                b_mean = np.mean([weights[n][l][1] for n in range(n_workers)], axis=0).astype(int)\n",
    "                average_weights[l] = [w_mean, b_mean]\n",
    "            # Set the computed average weights to the layers of the networks\n",
    "            for n in range(n_workers):\n",
    "                for l in range(n_layers):\n",
    "                    nets[n].layers[l+1].weights = average_weights[l][0]\n",
    "                    nets[n].layers[l+1].bias = average_weights[l][1]\n",
    "    # Compute the final test accuracy\n",
    "    test_acc = nets[0].test(x_test, y_test)\n",
    "    return {f\"{lr_inv}-{run}\": [test_acc, average_weights]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single-Layer Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sl_train(run, lr_inv, train_batches, x_test, y_test):\n",
    "    # Net structure\n",
    "    nets, n_layers = [], 0\n",
    "    for n in range(n_workers):\n",
    "        net = Network()\n",
    "        net.add(FlattenLayer())\n",
    "        net.add(FCLayer(28*28, 200, desc=\"First\"))\n",
    "        net.add(FCLayer(200, 10, desc=\"Second\", last_layer=True))\n",
    "\n",
    "        net.layers[1].DFA_weights = DFA_weights[run]\n",
    "\n",
    "        nets.append(net)\n",
    "    \n",
    "    # Count number of trainable layers\n",
    "    for l in net.layers:\n",
    "        if hasattr(l, \"weights\"):\n",
    "            n_layers += 1\n",
    "\n",
    "    # Single-layers Training\n",
    "    # Loop over the buffers until all the dataset is used\n",
    "    for buffer in range(len(train_batches[0][0])//buffer_len):\n",
    "        # Repeat for the number of epochs \n",
    "        for _ in range(epochs):\n",
    "            # Loop over the mini batches in the buffer\n",
    "            for mini_batch in range(buffer_len//mini_batch_size):\n",
    "                # Compute the starting and ending index of the mini batch\n",
    "                idx_start_batch = buffer * buffer_len + mini_batch * mini_batch_size\n",
    "                idx_end_batch = idx_start_batch + mini_batch_size\n",
    "\n",
    "                # Train the network and save the weights for each layer to be averaged\n",
    "                weights, average_weights = [[] for _ in range(n_layers)], [[] for _ in range(n_layers)]\n",
    "\n",
    "                # Initialize the layer to be trained\n",
    "                train_layer = 0\n",
    "\n",
    "                for n in range(n_workers):\n",
    "                    if n%(n_workers//n_layers) == 0:\n",
    "                        train_layer += 1\n",
    "                    # Extract the correct mini batch data and target from the training dataset\n",
    "                    data, target = train_batches[n][0][idx_start_batch:idx_end_batch], train_batches[n][1][idx_start_batch:idx_end_batch]\n",
    "                    train_res = nets[n].federated_fit_single_layers(data, target, train_layer, lr_inv)\n",
    "                    weights[train_layer-1].append(train_res)\n",
    "            \n",
    "            # Average the weights of the layers\n",
    "            for l in range(n_layers):\n",
    "                w_mean = np.mean([weights[l][n][0] for n in range(n_workers//n_layers)], axis=0).astype(int)\n",
    "                b_mean = np.mean([weights[l][n][1] for n in range(n_workers//n_layers)], axis=0).astype(int)\n",
    "                average_weights[l] = [w_mean, b_mean]\n",
    "                \n",
    "            # Set the computed average weights to the layers of the networks\n",
    "            for n in range(n_workers):\n",
    "                for l in range(n_layers):\n",
    "                    nets[n].layers[l+1].weights = average_weights[l][0]\n",
    "                    nets[n].layers[l+1].bias = average_weights[l][1]\n",
    "    # Compute the final test accuracy\n",
    "    test_acc = nets[0].test(x_test, y_test)\n",
    "    return {f\"{lr_inv}-{run}\": [test_acc, average_weights]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "train_batches, x_test = prepare_dataset(x_train, y_train, x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TIFeD Full-Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallelization of the training procedure using joblib \n",
    "fn_train_res = Parallel(n_jobs=8)(delayed(fn_train)(run, lr_inv, train_batches, x_test, y_test) for lr_inv in lrs_inv for run in range(n_runs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorder and save the results\n",
    "fn_train_nets = {}\n",
    "for lr_idx in range(len(lrs_inv)):\n",
    "    accs, W = [], []\n",
    "    for run in range(n_runs):\n",
    "        # Extract the results\n",
    "        index, dict_key = n_runs*lr_idx+run, f\"{lrs_inv[lr_idx]}-{run}\"\n",
    "        accs.append(fn_train_res[index][dict_key][0]/100)\n",
    "        W.append(fn_train_res[index][dict_key][1])\n",
    "    fn_train_nets[lrs_inv[lr_idx]] = [accs, W]\n",
    "\n",
    "with open(f\"out/full_network/mnist_{n_workers}.pkl\", \"wb\") as f:\n",
    "    pickle.dump(fn_train_nets, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TIFeD Single-Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallelization of the training procedure using joblib \n",
    "sl_train_res = Parallel(n_jobs=8)(delayed(sl_train)(run, lr_inv, train_batches, x_test, y_test) for lr_inv in lrs_inv for run in range(n_runs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorder and save the results\n",
    "sl_train_nets = {}\n",
    "for lr_idx in range(len(lrs_inv)):\n",
    "    accs, W = [], []\n",
    "    for run in range(n_runs):\n",
    "        # Extract the results\n",
    "        index, dict_key = n_runs*lr_idx+run, f\"{lrs_inv[lr_idx]}-{run}\"\n",
    "        accs.append(sl_train_res[index][dict_key][0]/100)\n",
    "        W.append(sl_train_res[index][dict_key][1])\n",
    "    sl_train_nets[lrs_inv[lr_idx]] = [accs, W]\n",
    "\n",
    "with open(f\"out/single_layers/mnist_{n_workers}.pkl\", \"wb\") as f:\n",
    "    pickle.dump(sl_train_nets, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FashionMNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "train_batches, x_test = prepare_dataset(x_train, y_train, x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TIFeD Full-Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallelization of the training procedure using joblib \n",
    "fn_train_res = Parallel(n_jobs=8)(delayed(fn_train)(run, lr_inv, train_batches, x_test, y_test) for lr_inv in lrs_inv for run in range(n_runs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorder and save the results\n",
    "fn_train_nets = {}\n",
    "for lr_idx in range(len(lrs_inv)):\n",
    "    accs, W = [], []\n",
    "    for run in range(n_runs):\n",
    "        # Extract the results\n",
    "        index, dict_key = n_runs*lr_idx+run, f\"{lrs_inv[lr_idx]}-{run}\"\n",
    "        accs.append(fn_train_res[index][dict_key][0]/100)\n",
    "        W.append(fn_train_res[index][dict_key][1])\n",
    "    fn_train_nets[lrs_inv[lr_idx]] = [accs, W]\n",
    "\n",
    "with open(f\"out/full_network/fmnist_{n_workers}.pkl\", \"wb\") as f:\n",
    "    pickle.dump(fn_train_nets, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TIFeD Single-Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallelization of the training procedure using joblib \n",
    "sl_train_res = Parallel(n_jobs=8)(delayed(sl_train)(run, lr_inv, train_batches, x_test, y_test) for lr_inv in lrs_inv for run in range(n_runs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorder and save the results\n",
    "sl_train_nets = {}\n",
    "for lr_idx in range(len(lrs_inv)):\n",
    "    accs, W = [], []\n",
    "    for run in range(n_runs):\n",
    "        # Extract the results\n",
    "        index, dict_key = n_runs*lr_idx+run, f\"{lrs_inv[lr_idx]}-{run}\"\n",
    "        accs.append(sl_train_res[index][dict_key][0]/100)\n",
    "        W.append(sl_train_res[index][dict_key][1])\n",
    "    sl_train_nets[lrs_inv[lr_idx]] = [accs, W]\n",
    "\n",
    "with open(f\"out/single_layers/fmnist_{n_workers}.pkl\", \"wb\") as f:\n",
    "    pickle.dump(sl_train_nets, f)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNDVK1GUNs7ICExTzMIGeYt",
   "collapsed_sections": [],
   "name": "PyCrCNN_PocketNN_Training.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
